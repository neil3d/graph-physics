{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOnv7Niuds8O"
      },
      "source": [
        "# Training a Transformer GNN on a blood flow inside an aneurysm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixhac9m8dwP6"
      },
      "source": [
        "We start by cloning the repo, and installing the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnWIThoWd0GU",
        "outputId": "f47fe382-ec59-4a5b-e6c8-eadc8ed12aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'graph-physics'...\n",
            "remote: Enumerating objects: 633, done.\u001b[K\n",
            "remote: Counting objects: 100% (230/230), done.\u001b[K\n",
            "remote: Compressing objects: 100% (168/168), done.\u001b[K\n",
            "remote: Total 633 (delta 131), reused 141 (delta 62), pack-reused 403 (from 1)\u001b[K\n",
            "Receiving objects: 100% (633/633), 32.28 MiB | 24.74 MiB/s, done.\n",
            "Resolving deltas: 100% (345/345), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://API_KEY@github.com/DonsetPG/graph-physics.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVctAgp8iG1R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygIO2VAQd4hv",
        "outputId": "74380a61-2ab4-4109-8ecb-c54cfc8faf9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.4.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_scatter-2.1.2%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt24cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.4.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_sparse-0.6.18%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt24cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.4.0+cu121.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_cluster-1.6.3%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (1.26.4)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt24cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.4.0+cu121.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (989 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m989.8/989.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt24cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.11)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2024.12.14)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pLIA2jad5qU",
        "outputId": "bcafcf71-4d06-433a-d9a3-2a6d7889bccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting loguru==0.7.2\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru\n",
            "Successfully installed loguru-0.7.2\n",
            "Collecting autoflake==2.3.0\n",
            "  Downloading autoflake-2.3.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting pyflakes>=3.0.0 (from autoflake==2.3.0)\n",
            "  Downloading pyflakes-3.2.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Downloading autoflake-2.3.0-py3-none-any.whl (32 kB)\n",
            "Downloading pyflakes-3.2.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyflakes, autoflake\n",
            "Successfully installed autoflake-2.3.0 pyflakes-3.2.0\n",
            "Collecting pytest==8.0.1\n",
            "  Downloading pytest-8.0.1-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest==8.0.1) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest==8.0.1) (24.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from pytest==8.0.1) (1.5.0)\n",
            "Downloading pytest-8.0.1-py3-none-any.whl (333 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m334.0/334.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytest\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 8.3.4\n",
            "    Uninstalling pytest-8.3.4:\n",
            "      Successfully uninstalled pytest-8.3.4\n",
            "Successfully installed pytest-8.0.1\n",
            "Collecting meshio==5.3.5\n",
            "  Downloading meshio-5.3.5-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from meshio==5.3.5) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from meshio==5.3.5) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->meshio==5.3.5) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->meshio==5.3.5) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->meshio==5.3.5) (0.1.2)\n",
            "Downloading meshio-5.3.5-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.2/166.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: meshio\n",
            "Successfully installed meshio-5.3.5\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.12.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.69.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Collecting h5py==3.10.0\n",
            "  Downloading h5py-3.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from h5py==3.10.0) (1.26.4)\n",
            "Downloading h5py-3.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h5py\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.12.1\n",
            "    Uninstalling h5py-3.12.1:\n",
            "      Successfully uninstalled h5py-3.12.1\n",
            "Successfully installed h5py-3.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install loguru==0.7.2\n",
        "!pip install autoflake==2.3.0\n",
        "!pip install pytest==8.0.1\n",
        "!pip install meshio==5.3.5\n",
        "!pip install tensorflow\n",
        "!pip install h5py==3.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d30qN-QXd64o",
        "outputId": "bad0c6f8-ca4f-4a40-d641-ccc828554e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html\n",
            "Collecting dgl\n",
            "  Downloading https://data.dgl.ai/wheels/torch-2.4/cu124/dgl-2.4.0%2Bcu124-cp311-cp311-manylinux1_x86_64.whl (347.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.11/dist-packages (from dgl) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from dgl) (24.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dgl) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.10.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from dgl) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dgl) (4.67.1)\n",
            "Collecting torch<=2.4.0 (from dgl)\n",
            "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dgl) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dgl) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dgl) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Collecting triton==3.0.0 (from torch<=2.4.0->dgl)\n",
            "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<=2.4.0->dgl) (12.6.85)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->dgl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dgl) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dgl) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<=2.4.0->dgl) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<=2.4.0->dgl) (1.3.0)\n",
            "Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nccl-cu12, torch, dgl\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dgl-2.4.0+cu124 nvidia-nccl-cu12-2.20.5 torch-2.4.0 triton-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "60eb811879394e40886e9078c49126b3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0QQLl3gd8KS",
        "outputId": "eac8ae09-2041-46d8-acf2-366231ccee78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvista\n",
            "  Downloading pyvista-0.44.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting lightning\n",
            "  Downloading lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from pyvista) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pyvista) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pyvista) (11.1.0)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.11/dist-packages (from pyvista) (1.8.2)\n",
            "Requirement already satisfied: scooby>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from pyvista) (0.10.0)\n",
            "Collecting vtk<9.4.0 (from pyvista)\n",
            "  Downloading vtk-9.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyvista) (4.12.2)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.10.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (24.2)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (2.4.0)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.67.1)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.5)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.11/dist-packages (from wandb[media]) (3.6.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from wandb[media]) (2.36.1)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (from wandb[media]) (1.0.3)\n",
            "Requirement already satisfied: plotly>=5.18.0 in /usr/local/lib/python3.11/dist-packages (from wandb[media]) (5.24.1)\n",
            "Collecting rdkit (from wandb[media])\n",
            "  Downloading rdkit-2024.9.4-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (from wandb[media]) (0.13.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.11)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.1->pyvista) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.1->pyvista) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.1->pyvista) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.1->pyvista) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.1->pyvista) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.1->pyvista) (2.8.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.18.0->wandb[media]) (9.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<4.0,>=2.1.0->lightning) (12.6.85)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from bokeh->wandb[media]) (2.2.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.11/dist-packages (from bokeh->wandb[media]) (6.3.3)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh->wandb[media]) (2024.9.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy->wandb[media]) (4.4.2)\n",
            "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy->wandb[media]) (0.5.1)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy->wandb[media]) (0.1.10)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile->wandb[media]) (1.17.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile->wandb[media]) (2.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->bokeh->wandb[media]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->bokeh->wandb[media]) (2024.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Downloading pyvista-0.44.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vtk-9.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (92.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdkit-2024.9.4-cp311-cp311-manylinux_2_28_x86_64.whl (34.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.2/34.2 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit, lightning-utilities, vtk, torchmetrics, pyvista, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.5.0.post0 lightning-utilities-0.11.9 pytorch-lightning-2.5.0.post0 pyvista-0.44.2 rdkit-2024.9.4 torchmetrics-1.6.1 vtk-9.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyvista lightning==2.5.0 wandb \"wandb[media]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning==2.5.0 torchmetrics==1.6.3"
      ],
      "metadata": {
        "id": "9gVUGlPgwD-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cx7O5Mnd8qQ"
      },
      "source": [
        "## Weights and Biases\n",
        "\n",
        "Make sure you are connected. If you don't have the webapp open, input your api key and press enter at the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "LnV4NsyLeFkT",
        "outputId": "b6e75eeb-ecc1-4d6e-b450-736c45c156f2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPAdXwMHeGFa"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "We now download the aneurysm dataset, as `.xmdf` files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxcABfQxGfWp",
        "outputId": "f861e4ab-ec04-410a-f3fe-e66cb814b78c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-16 09:45:51--  https://storage.googleapis.com/large-physics-model/datasets/aneurysm/coarse_03_dataset.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.137.207, 142.250.101.207, 142.251.2.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.137.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1123362206 (1.0G) [application/zip]\n",
            "Saving to: ‘coarse_03_dataset.zip’\n",
            "\n",
            "coarse_03_dataset.z 100%[===================>]   1.05G  23.9MB/s    in 45s     \n",
            "\n",
            "2025-01-16 09:46:37 (23.8 MB/s) - ‘coarse_03_dataset.zip’ saved [1123362206/1123362206]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O coarse_03_dataset.zip \"https://storage.googleapis.com/large-physics-model/datasets/aneurysm/coarse_03_dataset.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBNYsP86HL52",
        "outputId": "98fadbd7-5d7d-44ab-a9a0-2c27680a9996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  coarse_03_dataset.zip\n",
            "   creating: coarse_dataset/\n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_193-1.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_26.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_193-1.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_140.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_140.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_171.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_171.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_163-2.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_163-2.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_181.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_26.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_42-2.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_42-2.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_9-1.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_9-1.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_219.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_219.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_161.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_161.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_192.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_192.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_135.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_135.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_185.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_185.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_195.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_195.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_42-3.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_42-3.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_196-2.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_196-2.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_157.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_157.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_182-1.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_182-1.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_148.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_148.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_23.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_23.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_134.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_134.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_34.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_34.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_191.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_191.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_205.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_205.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_178.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_178.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_180.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_180.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_193-2.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_193-2.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_181.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_31.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_31.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_203.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_203.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_42-1.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_42-1.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_165.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_165.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_120.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_120.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_172.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_172.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_55.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_55.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_155.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_155.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_211.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_211.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_207.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_207.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_206.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_206.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_138.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_138.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_182-2.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_182-2.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_151.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_151.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_119-1.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_119-1.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_6.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_6.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_54-2.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_54-2.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_215.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_215.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_196-1.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_196-1.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_173.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_173.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_202.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_202.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_1.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_1.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_40.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_40.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_121.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_121.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_199.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_199.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_175.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_175.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_54-1.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_54-1.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_198-1.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_198-1.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_44.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_44.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_163-1.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_163-1.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_197.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_197.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_168-2.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_168-2.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_174.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_174.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_129.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_129.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_25.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_25.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_213.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_213.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_188.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_188.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_209.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_209.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_194.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_194.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_166.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_166.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_183.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_183.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_177.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_177.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_153.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_153.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_198-2.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_198-2.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_159.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_159.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_212.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_212.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_170.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_170.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_208.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_208.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_182-3.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_182-3.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_117.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_117.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_186-2.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_186-2.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_186-1.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_186-1.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_210.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_210.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_189.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_189.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_19.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_19.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_137.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_137.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_187.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_187.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_142.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_142.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_158.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_158.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_214.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_214.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_28.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_28.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_200.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_200.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_139.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_139.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_3.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_3.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_2.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_2.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_162.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_162.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_119-2.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_119-2.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_217.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_217.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_152.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_152.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_168-1.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_168-1.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_204.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_204.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_136.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_136.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_164.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_164.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_11.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_11.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_160.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_160.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_144-2.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_144-2.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_167.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_167.xdmf  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_128.h5  \n",
            "  inflating: coarse_dataset/AllFields_Resultats_MESH_128.xdmf  \n"
          ]
        }
      ],
      "source": [
        "!unzip \"coarse_03_dataset.zip\"\n",
        "!mv coarse_dataset WHOLE_DATASET/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UsFtlFaHW6F"
      },
      "source": [
        "Let's create our train and test split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6qIeEmhHY0j"
      },
      "outputs": [],
      "source": [
        "!mkdir dataset\n",
        "!mkdir dataset/train\n",
        "!mkdir dataset/test\n",
        "\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_202.h5\" dataset/test/\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_202.xdmf\" dataset/test/\n",
        "\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_128.h5\" dataset/test/\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_128.xdmf\" dataset/test/\n",
        "\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_212.h5\" dataset/test/\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_212.xdmf\" dataset/test/\n",
        "\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_167.h5\" dataset/test/\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_167.xdmf\" dataset/test/\n",
        "\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_135.h5\" dataset/test/\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_135.xdmf\" dataset/test/\n",
        "\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_160.h5\" dataset/test/\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_160.xdmf\" dataset/test/\n",
        "\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_205.h5\" dataset/test/\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_205.xdmf\" dataset/test/\n",
        "\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_164.h5\" dataset/test/\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_164.xdmf\" dataset/test/\n",
        "\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_136.h5\" dataset/test/\n",
        "!mv \"WHOLE_DATASET/AllFields_Resultats_MESH_136.xdmf\" dataset/test/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQyexoXhHgeX"
      },
      "outputs": [],
      "source": [
        "!mv WHOLE_DATASET/* dataset/train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khzF-AyLeM_M"
      },
      "source": [
        "Finally, we place the dataset inside of our repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJbL7ZY2d9X7"
      },
      "outputs": [],
      "source": [
        "!mv dataset graph-physics/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_zNqynKeRTK"
      },
      "source": [
        "# Pyvista\n",
        "\n",
        "When running Pyvista vizualisation in Google Colab, we need to add some packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4AOZnPReWpa",
        "outputId": "ecfbdee6-422f-4301-a9da-664d869f5a41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 124565 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.12_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.12_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -qq xvfb\n",
        "!pip install pyvista panel -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUZmWXLyeXQA"
      },
      "source": [
        "To make sure they work, we need to run the following code:\n",
        "\n",
        "```python\n",
        "import os\n",
        "os.system('/usr/bin/Xvfb :99 -screen 0 1024x768x24 &')\n",
        "os.environ['DISPLAY'] = ':99'\n",
        "\n",
        "import panel as pn\n",
        "pn.extension('vtk')\n",
        "```\n",
        "\n",
        "on top of our training script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wC1DWqYeiv5"
      },
      "source": [
        "# Training Script\n",
        "\n",
        "By default, we use the following parameters:\n",
        "\n",
        "```\n",
        "python -m graphphysics.train \\\n",
        "            --training_parameters_path=training.json \\\n",
        "            --num_epochs=5 \\\n",
        "            --init_lr=0.001 \\\n",
        "            --batch_size=2 \\\n",
        "            --warmup=500 \\\n",
        "            --num_workers=0 \\\n",
        "            --prefetch_factor=0 \\\n",
        "            --model_save_path=model.ckpt \\\n",
        "            --no_edge_feature\n",
        "```\n",
        "\n",
        "Here, since we train a transformer model on an `.xdmf` dataset, we can increase the number of workers\n",
        "- `--num_workers` is set to 2\n",
        "- `--prefetch_factor=0` is set to 2\n",
        "- we need the `--no_edge_feature` flag\n",
        "\n",
        "We can udpate the `train.sh` file to use the right training file by moving the file `coarse_aneurysm.json` from `training_config/` to our main folder and use:\n",
        "\n",
        "```\n",
        "python -m graphphysics.train \\\n",
        "            --training_parameters_path=training_config/coarse_aneurysm.json \\\n",
        "            --num_epochs=15 \\\n",
        "            --init_lr=0.001 \\\n",
        "            --batch_size=2 \\\n",
        "            --warmup=1500 \\\n",
        "            --num_workers=2 \\\n",
        "            --prefetch_factor=2 \\\n",
        "            --model_save_path=model.ckpt \\\n",
        "            --use_previous_data \\\n",
        "            --previous_data_start 4 \\\n",
        "            --previous_data_end 7\n",
        "            --no_edge_feature\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-BMHHn5IWwc"
      },
      "source": [
        "# Custom functions\n",
        "\n",
        "We need to both build the node types for our dataset, and some extra features as well (acceleration, input flow, etc)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIt6WiLyIrfR"
      },
      "source": [
        "## Node type\n",
        "\n",
        "Let's replace, in `/external/aneurysm.py` the node type function with the following:\n",
        "\n",
        "```python\n",
        "def aneurysm_node_type(graph: Data) -> torch.Tensor:\n",
        "    v_x = graph.x[:, 0]\n",
        "\n",
        "    wall_mask = graph.x[:, 3].type(torch.bool)\n",
        "    node_type = torch.zeros(v_x.shape)\n",
        "\n",
        "    inflow_mask = torch.logical_and(graph.pos[:, 1] <= 0.001, graph.pos[:, 0] <= 0)\n",
        "\n",
        "    outflow_mask = torch.logical_and(graph.pos[:, 1] <= 0.001, graph.pos[:, 0] >= 0)\n",
        "\n",
        "    node_type[wall_mask] = NodeType.WALL_BOUNDARY\n",
        "    node_type[inflow_mask] = NodeType.INFLOW\n",
        "    node_type[outflow_mask] = NodeType.OUTFLOW\n",
        "\n",
        "    return node_type.to(device)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSg--ruJJF6-"
      },
      "source": [
        "## Features\n",
        "\n",
        "We can now build some features. Let's replace the `torch.cat` at the end of the `build_features` function with the following:\n",
        "\n",
        "\n",
        "```python\n",
        "velocity_norm = torch.norm(graph.x[:, 0:3], dim=1)\n",
        "velocity_norm = velocity_norm.to(device).unsqueeze(1)\n",
        "x_mask = [0, 1, 2, 4]   # Remove the wall_mask from inputs\n",
        "graph.x = graph.x[:, x_mask]\n",
        "graph.x = torch.cat(\n",
        "    (\n",
        "        graph.x,\n",
        "        acceleration,\n",
        "        graph.pos,\n",
        "        mean_next_accel.unsqueeze(1),\n",
        "        min_next_accel.unsqueeze(1),\n",
        "        max_next_accel.unsqueeze(1),\n",
        "        velocity_norm,\n",
        "        node_type.unsqueeze(1),\n",
        "    ),\n",
        "    dim=1,\n",
        ")\n",
        "```\n",
        "\n",
        "This will:\n",
        "- remove the wall mask we used to build inputs\n",
        "- add the norm of the velocity as a feature\n",
        "\n",
        "In the rest of the function, we compute statistics about the blood inflow as well as the acceleration.\n",
        "\n",
        "Note that we use the attribute `graph.previous_data` to be able to compute such acceleration. We need to tell our dataset to give us such previous data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ajrmMo4JiJD"
      },
      "source": [
        "## Previous Data\n",
        "\n",
        "To do so, in `train.py`, we set `use_previous_data` to `True` in both dataset;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc12aHUZJthl"
      },
      "source": [
        "## Using custom functions\n",
        "\n",
        "We can now import `build_features` and use it in train:\n",
        "\n",
        "`from graphphysics.external.aneurysm import build_features`\n",
        "\n",
        "and the preprocessing becomes:\n",
        "\n",
        "```python\n",
        "# Build preprocessing function\n",
        "    preprocessing = get_preprocessing(\n",
        "        param=parameters,\n",
        "        device=device,\n",
        "        use_edge_feature=use_edge_feature,\n",
        "        extra_node_features=build_features,\n",
        "    )\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XCBat6tKFzs"
      },
      "source": [
        "## Other remarks\n",
        "\n",
        "Our dataset only has 79 steps per trajectory. We thus need to update the list of index used for vizualisation. Let's set it to `[1,10,20]`.\n",
        "We also reduce the number of steps per log to in the `Trainer` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fs0_58ypFY1n",
        "outputId": "42044e9a-cdc7-48a8-e0ba-b42728de4b2b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-ba1dc3213b56>:6: UserWarning: Using Panel interactively in Colab notebooks requires the jupyter_bokeh package to be installed. Install it with:\n",
            "\n",
            "    !pip install jupyter_bokeh\n",
            "\n",
            "and try again.\n",
            "  pn.extension('vtk')\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "(function(root) {\n",
              "  function now() {\n",
              "    return new Date();\n",
              "  }\n",
              "\n",
              "  var force = true;\n",
              "  var py_version = '3.4.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n",
              "  var reloading = false;\n",
              "  var Bokeh = root.Bokeh;\n",
              "\n",
              "  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n",
              "    root._bokeh_timeout = Date.now() + 5000;\n",
              "    root._bokeh_failed_load = false;\n",
              "  }\n",
              "\n",
              "  function run_callbacks() {\n",
              "    try {\n",
              "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
              "        if (callback != null)\n",
              "          callback();\n",
              "      });\n",
              "    } finally {\n",
              "      delete root._bokeh_onload_callbacks;\n",
              "    }\n",
              "    console.debug(\"Bokeh: all callbacks have finished\");\n",
              "  }\n",
              "\n",
              "  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n",
              "    if (css_urls == null) css_urls = [];\n",
              "    if (js_urls == null) js_urls = [];\n",
              "    if (js_modules == null) js_modules = [];\n",
              "    if (js_exports == null) js_exports = {};\n",
              "\n",
              "    root._bokeh_onload_callbacks.push(callback);\n",
              "\n",
              "    if (root._bokeh_is_loading > 0) {\n",
              "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
              "      return null;\n",
              "    }\n",
              "    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n",
              "      run_callbacks();\n",
              "      return null;\n",
              "    }\n",
              "    if (!reloading) {\n",
              "      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
              "    }\n",
              "\n",
              "    function on_load() {\n",
              "      root._bokeh_is_loading--;\n",
              "      if (root._bokeh_is_loading === 0) {\n",
              "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
              "        run_callbacks()\n",
              "      }\n",
              "    }\n",
              "    window._bokeh_on_load = on_load\n",
              "\n",
              "    function on_error() {\n",
              "      console.error(\"failed to load \" + url);\n",
              "    }\n",
              "\n",
              "    var skip = [];\n",
              "    if (window.requirejs) {\n",
              "      window.requirejs.config({'packages': {}, 'paths': {'vtk': 'https://cdn.jsdelivr.net/npm/vtk.js@30.1.0/vtk'}, 'shim': {'vtk': {'exports': 'vtk'}}});\n",
              "      require([\"vtk\"], function() {\n",
              "\ton_load()\n",
              "      })\n",
              "      root._bokeh_is_loading = css_urls.length + 1;\n",
              "    } else {\n",
              "      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n",
              "    }\n",
              "\n",
              "    var existing_stylesheets = []\n",
              "    var links = document.getElementsByTagName('link')\n",
              "    for (var i = 0; i < links.length; i++) {\n",
              "      var link = links[i]\n",
              "      if (link.href != null) {\n",
              "\texisting_stylesheets.push(link.href)\n",
              "      }\n",
              "    }\n",
              "    for (var i = 0; i < css_urls.length; i++) {\n",
              "      var url = css_urls[i];\n",
              "      if (existing_stylesheets.indexOf(url) !== -1) {\n",
              "\ton_load()\n",
              "\tcontinue;\n",
              "      }\n",
              "      const element = document.createElement(\"link\");\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.rel = \"stylesheet\";\n",
              "      element.type = \"text/css\";\n",
              "      element.href = url;\n",
              "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
              "      document.body.appendChild(element);\n",
              "    }    if (((window.vtk !== undefined) && (!(window.vtk instanceof HTMLElement))) || window.requirejs) {\n",
              "      var urls = ['https://cdn.holoviz.org/panel/1.4.5/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js'];\n",
              "      for (var i = 0; i < urls.length; i++) {\n",
              "        skip.push(urls[i])\n",
              "      }\n",
              "    }    var existing_scripts = []\n",
              "    var scripts = document.getElementsByTagName('script')\n",
              "    for (var i = 0; i < scripts.length; i++) {\n",
              "      var script = scripts[i]\n",
              "      if (script.src != null) {\n",
              "\texisting_scripts.push(script.src)\n",
              "      }\n",
              "    }\n",
              "    for (var i = 0; i < js_urls.length; i++) {\n",
              "      var url = js_urls[i];\n",
              "      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n",
              "\tif (!window.requirejs) {\n",
              "\t  on_load();\n",
              "\t}\n",
              "\tcontinue;\n",
              "      }\n",
              "      var element = document.createElement('script');\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.async = false;\n",
              "      element.src = url;\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "    for (var i = 0; i < js_modules.length; i++) {\n",
              "      var url = js_modules[i];\n",
              "      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n",
              "\tif (!window.requirejs) {\n",
              "\t  on_load();\n",
              "\t}\n",
              "\tcontinue;\n",
              "      }\n",
              "      var element = document.createElement('script');\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.async = false;\n",
              "      element.src = url;\n",
              "      element.type = \"module\";\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "    for (const name in js_exports) {\n",
              "      var url = js_exports[name];\n",
              "      if (skip.indexOf(url) >= 0 || root[name] != null) {\n",
              "\tif (!window.requirejs) {\n",
              "\t  on_load();\n",
              "\t}\n",
              "\tcontinue;\n",
              "      }\n",
              "      var element = document.createElement('script');\n",
              "      element.onerror = on_error;\n",
              "      element.async = false;\n",
              "      element.type = \"module\";\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      element.textContent = `\n",
              "      import ${name} from \"${url}\"\n",
              "      window.${name} = ${name}\n",
              "      window._bokeh_on_load()\n",
              "      `\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "    if (!js_urls.length && !js_modules.length) {\n",
              "      on_load()\n",
              "    }\n",
              "  };\n",
              "\n",
              "  function inject_raw_css(css) {\n",
              "    const element = document.createElement(\"style\");\n",
              "    element.appendChild(document.createTextNode(css));\n",
              "    document.body.appendChild(element);\n",
              "  }\n",
              "\n",
              "  var js_urls = [\"https://cdn.holoviz.org/panel/1.4.5/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.holoviz.org/panel/1.4.5/dist/panel.min.js\"];\n",
              "  var js_modules = [];\n",
              "  var js_exports = {};\n",
              "  var css_urls = [];\n",
              "  var inline_js = [    function(Bokeh) {\n",
              "      Bokeh.set_log_level(\"info\");\n",
              "    },\n",
              "function(Bokeh) {} // ensure no trailing comma for IE\n",
              "  ];\n",
              "\n",
              "  function run_inline_js() {\n",
              "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
              "      for (var i = 0; i < inline_js.length; i++) {\n",
              "\ttry {\n",
              "          inline_js[i].call(root, root.Bokeh);\n",
              "\t} catch(e) {\n",
              "\t  if (!reloading) {\n",
              "\t    throw e;\n",
              "\t  }\n",
              "\t}\n",
              "      }\n",
              "      // Cache old bokeh versions\n",
              "      if (Bokeh != undefined && !reloading) {\n",
              "\tvar NewBokeh = root.Bokeh;\n",
              "\tif (Bokeh.versions === undefined) {\n",
              "\t  Bokeh.versions = new Map();\n",
              "\t}\n",
              "\tif (NewBokeh.version !== Bokeh.version) {\n",
              "\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n",
              "\t}\n",
              "\troot.Bokeh = Bokeh;\n",
              "      }} else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(run_inline_js, 100);\n",
              "    } else if (!root._bokeh_failed_load) {\n",
              "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
              "      root._bokeh_failed_load = true;\n",
              "    }\n",
              "    root._bokeh_is_initializing = false\n",
              "  }\n",
              "\n",
              "  function load_or_wait() {\n",
              "    // Implement a backoff loop that tries to ensure we do not load multiple\n",
              "    // versions of Bokeh and its dependencies at the same time.\n",
              "    // In recent versions we use the root._bokeh_is_initializing flag\n",
              "    // to determine whether there is an ongoing attempt to initialize\n",
              "    // bokeh, however for backward compatibility we also try to ensure\n",
              "    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n",
              "    // before older versions are fully initialized.\n",
              "    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n",
              "      root._bokeh_is_initializing = false;\n",
              "      root._bokeh_onload_callbacks = undefined;\n",
              "      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n",
              "      load_or_wait();\n",
              "    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n",
              "      setTimeout(load_or_wait, 100);\n",
              "    } else {\n",
              "      root._bokeh_is_initializing = true\n",
              "      root._bokeh_onload_callbacks = []\n",
              "      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n",
              "      if (!reloading && !bokeh_loaded) {\n",
              "\troot.Bokeh = undefined;\n",
              "      }\n",
              "      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n",
              "\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
              "\trun_inline_js();\n",
              "      });\n",
              "    }\n",
              "  }\n",
              "  // Give older versions of the autoload script a head-start to ensure\n",
              "  // they initialize before we start loading newer version.\n",
              "  setTimeout(load_or_wait, 100)\n",
              "}(window));"
            ],
            "application/vnd.holoviews_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'vtk': 'https://cdn.jsdelivr.net/npm/vtk.js@30.1.0/vtk'}, 'shim': {'vtk': {'exports': 'vtk'}}});\n      require([\"vtk\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 1;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window.vtk !== undefined) && (!(window.vtk instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.4.5/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.holoviz.org/panel/1.4.5/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.holoviz.org/panel/1.4.5/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
              "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
              "}\n",
              "\n",
              "\n",
              "    function JupyterCommManager() {\n",
              "    }\n",
              "\n",
              "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
              "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
              "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
              "        comm_manager.register_target(comm_id, function(comm) {\n",
              "          comm.on_msg(msg_handler);\n",
              "        });\n",
              "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
              "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
              "          comm.onMsg = msg_handler;\n",
              "        });\n",
              "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
              "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
              "          var messages = comm.messages[Symbol.asyncIterator]();\n",
              "          function processIteratorResult(result) {\n",
              "            var message = result.value;\n",
              "            console.log(message)\n",
              "            var content = {data: message.data, comm_id};\n",
              "            var buffers = []\n",
              "            for (var buffer of message.buffers || []) {\n",
              "              buffers.push(new DataView(buffer))\n",
              "            }\n",
              "            var metadata = message.metadata || {};\n",
              "            var msg = {content, buffers, metadata}\n",
              "            msg_handler(msg);\n",
              "            return messages.next().then(processIteratorResult);\n",
              "          }\n",
              "          return messages.next().then(processIteratorResult);\n",
              "        })\n",
              "      }\n",
              "    }\n",
              "\n",
              "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
              "      if (comm_id in window.PyViz.comms) {\n",
              "        return window.PyViz.comms[comm_id];\n",
              "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
              "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
              "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
              "        if (msg_handler) {\n",
              "          comm.on_msg(msg_handler);\n",
              "        }\n",
              "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
              "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
              "        comm.open();\n",
              "        if (msg_handler) {\n",
              "          comm.onMsg = msg_handler;\n",
              "        }\n",
              "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
              "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
              "        comm_promise.then((comm) => {\n",
              "          window.PyViz.comms[comm_id] = comm;\n",
              "          if (msg_handler) {\n",
              "            var messages = comm.messages[Symbol.asyncIterator]();\n",
              "            function processIteratorResult(result) {\n",
              "              var message = result.value;\n",
              "              var content = {data: message.data};\n",
              "              var metadata = message.metadata || {comm_id};\n",
              "              var msg = {content, metadata}\n",
              "              msg_handler(msg);\n",
              "              return messages.next().then(processIteratorResult);\n",
              "            }\n",
              "            return messages.next().then(processIteratorResult);\n",
              "          }\n",
              "        }) \n",
              "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
              "          return comm_promise.then((comm) => {\n",
              "            comm.send(data, metadata, buffers, disposeOnDone);\n",
              "          });\n",
              "        };\n",
              "        var comm = {\n",
              "          send: sendClosure\n",
              "        };\n",
              "      }\n",
              "      window.PyViz.comms[comm_id] = comm;\n",
              "      return comm;\n",
              "    }\n",
              "    window.PyViz.comm_manager = new JupyterCommManager();\n",
              "    \n",
              "\n",
              "\n",
              "var JS_MIME_TYPE = 'application/javascript';\n",
              "var HTML_MIME_TYPE = 'text/html';\n",
              "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
              "var CLASS_NAME = 'output';\n",
              "\n",
              "/**\n",
              " * Render data to the DOM node\n",
              " */\n",
              "function render(props, node) {\n",
              "  var div = document.createElement(\"div\");\n",
              "  var script = document.createElement(\"script\");\n",
              "  node.appendChild(div);\n",
              "  node.appendChild(script);\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle when a new output is added\n",
              " */\n",
              "function handle_add_output(event, handle) {\n",
              "  var output_area = handle.output_area;\n",
              "  var output = handle.output;\n",
              "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
              "    return\n",
              "  }\n",
              "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
              "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
              "  if (id !== undefined) {\n",
              "    var nchildren = toinsert.length;\n",
              "    var html_node = toinsert[nchildren-1].children[0];\n",
              "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "    var scripts = [];\n",
              "    var nodelist = html_node.querySelectorAll(\"script\");\n",
              "    for (var i in nodelist) {\n",
              "      if (nodelist.hasOwnProperty(i)) {\n",
              "        scripts.push(nodelist[i])\n",
              "      }\n",
              "    }\n",
              "\n",
              "    scripts.forEach( function (oldScript) {\n",
              "      var newScript = document.createElement(\"script\");\n",
              "      var attrs = [];\n",
              "      var nodemap = oldScript.attributes;\n",
              "      for (var j in nodemap) {\n",
              "        if (nodemap.hasOwnProperty(j)) {\n",
              "          attrs.push(nodemap[j])\n",
              "        }\n",
              "      }\n",
              "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
              "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
              "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
              "    });\n",
              "    if (JS_MIME_TYPE in output.data) {\n",
              "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
              "    }\n",
              "    output_area._hv_plot_id = id;\n",
              "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
              "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
              "    } else {\n",
              "      window.PyViz.plot_index[id] = null;\n",
              "    }\n",
              "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
              "    var bk_div = document.createElement(\"div\");\n",
              "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "    var script_attrs = bk_div.children[0].attributes;\n",
              "    for (var i = 0; i < script_attrs.length; i++) {\n",
              "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
              "    }\n",
              "    // store reference to server id on output_area\n",
              "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
              "  }\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle when an output is cleared or removed\n",
              " */\n",
              "function handle_clear_output(event, handle) {\n",
              "  var id = handle.cell.output_area._hv_plot_id;\n",
              "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
              "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
              "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
              "  if (server_id !== null) {\n",
              "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
              "    return;\n",
              "  } else if (comm !== null) {\n",
              "    comm.send({event_type: 'delete', 'id': id});\n",
              "  }\n",
              "  delete PyViz.plot_index[id];\n",
              "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
              "    var doc = window.Bokeh.index[id].model.document\n",
              "    doc.clear();\n",
              "    const i = window.Bokeh.documents.indexOf(doc);\n",
              "    if (i > -1) {\n",
              "      window.Bokeh.documents.splice(i, 1);\n",
              "    }\n",
              "  }\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle kernel restart event\n",
              " */\n",
              "function handle_kernel_cleanup(event, handle) {\n",
              "  delete PyViz.comms[\"hv-extension-comm\"];\n",
              "  window.PyViz.plot_index = {}\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle update_display_data messages\n",
              " */\n",
              "function handle_update_output(event, handle) {\n",
              "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
              "  handle_add_output(event, handle)\n",
              "}\n",
              "\n",
              "function register_renderer(events, OutputArea) {\n",
              "  function append_mime(data, metadata, element) {\n",
              "    // create a DOM node to render to\n",
              "    var toinsert = this.create_output_subarea(\n",
              "    metadata,\n",
              "    CLASS_NAME,\n",
              "    EXEC_MIME_TYPE\n",
              "    );\n",
              "    this.keyboard_manager.register_events(toinsert);\n",
              "    // Render to node\n",
              "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
              "    render(props, toinsert[0]);\n",
              "    element.append(toinsert);\n",
              "    return toinsert\n",
              "  }\n",
              "\n",
              "  events.on('output_added.OutputArea', handle_add_output);\n",
              "  events.on('output_updated.OutputArea', handle_update_output);\n",
              "  events.on('clear_output.CodeCell', handle_clear_output);\n",
              "  events.on('delete.Cell', handle_clear_output);\n",
              "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
              "\n",
              "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
              "    safe: true,\n",
              "    index: 0\n",
              "  });\n",
              "}\n",
              "\n",
              "if (window.Jupyter !== undefined) {\n",
              "  try {\n",
              "    var events = require('base/js/events');\n",
              "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
              "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
              "      register_renderer(events, OutputArea);\n",
              "    }\n",
              "  } catch(err) {\n",
              "  }\n",
              "}\n"
            ],
            "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>*[data-root-id],\n",
              "*[data-root-id] > * {\n",
              "  box-sizing: border-box;\n",
              "  font-family: var(--jp-ui-font-family);\n",
              "  font-size: var(--jp-ui-font-size1);\n",
              "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
              "}\n",
              "\n",
              "/* Override VSCode background color */\n",
              ".cell-output-ipywidget-background:has(\n",
              "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
              "  ),\n",
              ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
              "  background-color: transparent !important;\n",
              "}\n",
              "</style>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.holoviews_exec.v0+json": "",
            "text/html": [
              "<div id='6a604f91-b4c5-4908-a86f-8ec34dfa25ad'>\n",
              "  <div id=\"bf52be0e-4221-4414-bd8e-57828234fb52\" data-root-id=\"6a604f91-b4c5-4908-a86f-8ec34dfa25ad\" style=\"display: contents;\"></div>\n",
              "</div>\n",
              "<script type=\"application/javascript\">(function(root) {\n",
              "  var docs_json = {\"f29c3d7e-72ec-4ed0-aa2c-8638fc315251\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"6a604f91-b4c5-4908-a86f-8ec34dfa25ad\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"cee00ac1-234e-4af8-a0ca-76feb5fe826c\",\"attributes\":{\"plot_id\":\"6a604f91-b4c5-4908-a86f-8ec34dfa25ad\",\"comm_id\":\"e63586e90cd74c0abad103bed9ac2560\",\"client_comm_id\":\"b5f1c3b6a69b4ad28dd40e66f6b76e4c\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
              "  var render_items = [{\"docid\":\"f29c3d7e-72ec-4ed0-aa2c-8638fc315251\",\"roots\":{\"6a604f91-b4c5-4908-a86f-8ec34dfa25ad\":\"bf52be0e-4221-4414-bd8e-57828234fb52\"},\"root_ids\":[\"6a604f91-b4c5-4908-a86f-8ec34dfa25ad\"]}];\n",
              "  var docs = Object.values(docs_json)\n",
              "  if (!docs) {\n",
              "    return\n",
              "  }\n",
              "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
              "  async function embed_document(root) {\n",
              "    var Bokeh = get_bokeh(root)\n",
              "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
              "    for (const render_item of render_items) {\n",
              "      for (const root_id of render_item.root_ids) {\n",
              "\tconst id_el = document.getElementById(root_id)\n",
              "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
              "\t  const root_el = id_el.children[0]\n",
              "\t  root_el.id = root_el.id + '-rendered'\n",
              "\t  for (const child of root_el.children) {\n",
              "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
              "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
              "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
              "\t  }\n",
              "\t}\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "  function get_bokeh(root) {\n",
              "    if (root.Bokeh === undefined) {\n",
              "      return null\n",
              "    } else if (root.Bokeh.version !== py_version) {\n",
              "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
              "\treturn null\n",
              "      }\n",
              "      return root.Bokeh.versions.get(py_version);\n",
              "    } else if (root.Bokeh.version === py_version) {\n",
              "      return root.Bokeh\n",
              "    }\n",
              "    return null\n",
              "  }\n",
              "  function is_loaded(root) {\n",
              "    var Bokeh = get_bokeh(root)\n",
              "    return (Bokeh != null && Bokeh.Panel !== undefined && ( root.vtk !== undefined) && ( root.vtk !== undefined) && ( root.vtk !== undefined) && ( root.vtk !== undefined) && ( root.vtk !== undefined) && ( root.vtk !== undefined) && ( root.vtk !== undefined) && ( root.vtk !== undefined) && ( root.vtk !== undefined) && ( root.vtk !== undefined) && ( root.vtk !== undefined) && ( root.vtk !== undefined) && ( root.vtk !== undefined))\n",
              "  }\n",
              "  if (is_loaded(root)) {\n",
              "    embed_document(root);\n",
              "  } else {\n",
              "    var attempts = 0;\n",
              "    var timer = setInterval(function(root) {\n",
              "      if (is_loaded(root)) {\n",
              "        clearInterval(timer);\n",
              "        embed_document(root);\n",
              "      } else if (document.readyState == \"complete\") {\n",
              "        attempts++;\n",
              "        if (attempts > 200) {\n",
              "          clearInterval(timer);\n",
              "\t  var Bokeh = get_bokeh(root)\n",
              "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
              "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
              "\t  } else {\n",
              "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
              "\t    embed_document(root)\n",
              "\t  }\n",
              "        }\n",
              "      }\n",
              "    }, 25, root)\n",
              "  }\n",
              "})(window);</script>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2024-10-15 08:23:37.697\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mOpening training parameters from training.json\u001b[0m\n",
            "\u001b[32m2024-10-15 08:23:37.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m149\u001b[0m - \u001b[1mInitializing new model\u001b[0m\n",
            "EncodeTransformDecode(\n",
            "  (nodes_encoder): Sequential(\n",
            "    (0): Linear(in_features=23, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (7): RMSNorm()\n",
            "  )\n",
            "  (decode_module): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=64, out_features=3, bias=True)\n",
            "  )\n",
            "  (processor_list): ModuleList(\n",
            "    (0-9): 10 x Transformer(\n",
            "      (attention): Attention(\n",
            "        (q_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (k_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (v_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "      )\n",
            "      (activation): ReLU()\n",
            "      (norm1): RMSNorm()\n",
            "      (norm2): RMSNorm()\n",
            "      (gated_mlp): Sequential(\n",
            "        (0): RMSNorm()\n",
            "        (1): GatedMLP(\n",
            "          (linear1): Linear(in_features=64, out_features=192, bias=True)\n",
            "          (linear2): Linear(in_features=64, out_features=192, bias=True)\n",
            "          (activation): GELU(approximate='none')\n",
            "        )\n",
            "        (2): Linear(in_features=192, out_features=64, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaul-garnier-97\u001b[0m (\u001b[33mpaul-garnier-97-cemef\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/graph-physics/wandb/run-20241015_082338-wvkxa75u\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdevoted-sun-12\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/paul-garnier-97-cemef/coarse-aneurysm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/paul-garnier-97-cemef/coarse-aneurysm/runs/wvkxa75u\u001b[0m\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "\u001b[32m2024-10-15 08:23:38.579\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m191\u001b[0m - \u001b[32m\u001b[1mStarting training\u001b[0m\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type      | Params | Mode \n",
            "--------------------------------------------\n",
            "0 | model | Simulator | 568 K  | train\n",
            "1 | loss  | L2Loss    | 0      | train\n",
            "--------------------------------------------\n",
            "568 K     Trainable params\n",
            "0         Non-trainable params\n",
            "568 K     Total params\n",
            "2.273     Total estimated model params size (MB)\n",
            "183       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Sanity Checking DataLoader 0: 100% 2/2 [00:02<00:00,  1.37s/it]ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
            "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
            "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
            "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
            "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
            "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
            "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
            "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
            "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
            "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
            "\u001b[32m2024-10-15 08:24:05.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgraphphysics.training.lightning_module\u001b[0m:\u001b[36mon_validation_epoch_end\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mValidation Trajectory saved at meshes/epoch_0\u001b[0m\n",
            "Epoch 0: 100% 3744/3744 [05:11<00:00, 12.01it/s, v_num=a75u, train_loss_step=0.693]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m2024-10-15 08:29:54.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgraphphysics.training.lightning_module\u001b[0m:\u001b[36mon_validation_epoch_end\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mValidation Trajectory saved at meshes/epoch_0\u001b[0m\n",
            "Epoch 1: 100% 3744/3744 [05:09<00:00, 12.10it/s, v_num=a75u, train_loss_step=0.177, val_loss_step=1.15e+3, val_loss_epoch=993.0, val_all_rollout_rmse=24.60, train_loss_epoch=0.669]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m2024-10-15 08:35:42.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgraphphysics.training.lightning_module\u001b[0m:\u001b[36mon_validation_epoch_end\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mValidation Trajectory saved at meshes/epoch_1\u001b[0m\n",
            "Epoch 2: 100% 3744/3744 [05:09<00:00, 12.08it/s, v_num=a75u, train_loss_step=0.296, val_loss_step=861.0, val_loss_epoch=674.0, val_all_rollout_rmse=20.20, train_loss_epoch=0.277]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m2024-10-15 08:41:30.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgraphphysics.training.lightning_module\u001b[0m:\u001b[36mon_validation_epoch_end\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mValidation Trajectory saved at meshes/epoch_2\u001b[0m\n",
            "Epoch 3: 100% 3744/3744 [05:09<00:00, 12.08it/s, v_num=a75u, train_loss_step=0.0994, val_loss_step=1.78e+3, val_loss_epoch=979.0, val_all_rollout_rmse=24.50, train_loss_epoch=0.222]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m2024-10-15 08:47:20.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgraphphysics.training.lightning_module\u001b[0m:\u001b[36mon_validation_epoch_end\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mValidation Trajectory saved at meshes/epoch_3\u001b[0m\n",
            "Epoch 4: 100% 3744/3744 [05:11<00:00, 12.03it/s, v_num=a75u, train_loss_step=0.145, val_loss_step=1.37e+3, val_loss_epoch=1.18e+3, val_all_rollout_rmse=26.80, train_loss_epoch=0.189] \n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m2024-10-15 08:53:12.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgraphphysics.training.lightning_module\u001b[0m:\u001b[36mon_validation_epoch_end\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mValidation Trajectory saved at meshes/epoch_4\u001b[0m\n",
            "Epoch 5: 100% 3744/3744 [05:11<00:00, 12.02it/s, v_num=a75u, train_loss_step=0.0742, val_loss_step=667.0, val_loss_epoch=441.0, val_all_rollout_rmse=16.40, train_loss_epoch=0.170]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m2024-10-15 08:59:04.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgraphphysics.training.lightning_module\u001b[0m:\u001b[36mon_validation_epoch_end\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mValidation Trajectory saved at meshes/epoch_5\u001b[0m\n",
            "Epoch 6: 100% 3744/3744 [05:11<00:00, 12.03it/s, v_num=a75u, train_loss_step=1.820, val_loss_step=404.0, val_loss_epoch=311.0, val_all_rollout_rmse=13.80, train_loss_epoch=0.153] \n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m2024-10-15 09:04:54.790\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgraphphysics.training.lightning_module\u001b[0m:\u001b[36mon_validation_epoch_end\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mValidation Trajectory saved at meshes/epoch_6\u001b[0m\n",
            "Epoch 7: 100% 3744/3744 [05:18<00:00, 11.74it/s, v_num=a75u, train_loss_step=0.0473, val_loss_step=341.0, val_loss_epoch=209.0, val_all_rollout_rmse=11.30, train_loss_epoch=0.140]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m2024-10-15 09:10:55.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgraphphysics.training.lightning_module\u001b[0m:\u001b[36mon_validation_epoch_end\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mValidation Trajectory saved at meshes/epoch_7\u001b[0m\n",
            "Epoch 8: 100% 3744/3744 [05:17<00:00, 11.79it/s, v_num=a75u, train_loss_step=0.0458, val_loss_step=336.0, val_loss_epoch=180.0, val_all_rollout_rmse=10.50, train_loss_epoch=0.128]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m2024-10-15 09:16:52.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgraphphysics.training.lightning_module\u001b[0m:\u001b[36mon_validation_epoch_end\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mValidation Trajectory saved at meshes/epoch_8\u001b[0m\n",
            "Epoch 9: 100% 3744/3744 [05:11<00:00, 12.01it/s, v_num=a75u, train_loss_step=0.120, val_loss_step=45.20, val_loss_epoch=71.80, val_all_rollout_rmse=6.610, train_loss_epoch=0.117]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m2024-10-15 09:22:43.899\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgraphphysics.training.lightning_module\u001b[0m:\u001b[36mon_validation_epoch_end\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mValidation Trajectory saved at meshes/epoch_9\u001b[0m\n",
            "Epoch 10:  15% 560/3744 [00:46<04:26, 11.97it/s, v_num=a75u, train_loss_step=0.0688, val_loss_step=31.80, val_loss_epoch=41.20, val_all_rollout_rmse=5.010, train_loss_epoch=0.108]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.system('/usr/bin/Xvfb :99 -screen 0 1024x768x24 &')\n",
        "os.environ['DISPLAY'] = ':99'\n",
        "\n",
        "import panel as pn\n",
        "pn.extension('vtk')\n",
        "!cd graph-physics && sh train.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hasw92WwfXrW"
      },
      "source": [
        "# Final Vizualisation\n",
        "\n",
        "During training, you can also check some meshes inside of `graph-physics/meshes/epoch_{n}`\n",
        "\n",
        "At the end of the training, we can zip it to make it easier to download it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMiONBy-fkNu"
      },
      "outputs": [],
      "source": [
        "!zip -r aneurysm.zip graph-physics/meshes/epoch_10/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}